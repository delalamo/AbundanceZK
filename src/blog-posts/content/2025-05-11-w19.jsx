import { useRef } from 'react';

import { useInTextFootnoteNumbering } from '../../hooks/useInTextFootnoteNumbering';
import { useCitationData } from '../../hooks/useCitationData';
import { FootnoteList } from '../../components/FootnoteList';
import MolstarViewer from '../../components/MolstarViewer'; 

const footnotesConfig = [
    { id: 'fn1', doi: '10.1101/2025.04.30.651414' },
    { id: 'fn2', doi: '10.1101/2025.02.03.636309' },
    { id: 'fn3', doi: '10.1101/2025.04.14.648669' },
    { id: 'fn4', doi: '10.1101/2025.04.07.647682' },
    { id: 'fn5', doi: '10.1101/2025.05.01.651643' },
    { id: 'fn6', doi: '10.1073/pnas.0511156103' },
    { id: 'fn7', doi: '10.1126/science.1208351' },
    { id: 'fn8', doi: '10.1101/2024.12.05.626885' },
    { id: 'fn9', doi: '10.1101/2025.03.09.642148' },
    { id: 'fn11', doi: '10.1073/pnas.2406285121' },
    { id: 'fn12', doi: '10.1101/2024.02.05.578959' },
    { id: 'fn13', doi: '10.1101/2024.10.03.616542' }
];

export const post = {
  id: '2025-05-11-zk',
  title: '2025 week 19 updates',
  date: '2025-05-11',
  category: 'Zettelkasten updates',
  excerpt: 'RNA structure prediction, NMR structural ensembles, and protein language model scaling laws.',
  content: (() => {
    const ContentComponent = () => {
      const contentRef = useRef(null);
      const idToNumberMap = useInTextFootnoteNumbering(contentRef);
      const { citationsData, isLoading, error } = useCitationData(footnotesConfig, { template: 'apa' });

      // Define nmrViewerOptions for this specific Molstar instance
      const nmrViewerOptions = {
        landscape: true, // This was implied by your original HTML for #myViewer
        // sequencePanel: true, // Example: if you want sequence panel visible by default
        // hideCanvasControls: [], // Example: to show all canvas controls
      };

      return (
        <div className="content-container" ref={contentRef}>
          <p><i>This post includes updates from various papers and preprints that have been released this week, and aims to extract some key points rather than summarize them in their entirety.</i></p>
          
          <h4>All-atom structure prediction of RNA is driven by memorization</h4>
          <p>Models of RNA structure generated by the all-atom protein structure prediction method AlphaFold3 are only as good as the most similar RNA structures in the training set<sup className="footnote-ref">[<a href="#fn1" id="fnref1"></a>]</sup>, consistent with memorization of training data.</p>

          <img src="/assets/post_images/2025_05_11/2025_05_11_A.png" alt="RNA structure prediction comparison" width="400" style={{ display: 'block', marginLeft: 'auto', marginRight: 'auto', height: 'auto' }} />

          <p>Although this generation of methods (AF3, Boltz, Chai, etc) can produce high-quality models of proteins, molecules without coevolutionary signatures are another matter. Earlier this year, Skrinjar et al 2025<sup className="footnote-ref">[<a href="#fn2" id="fnref2"></a>]</sup> found that novel ligands were incorrectly docked far more often than training set ligands. Other reports have shown that AF3 cannot model changes in conformational equilibria resulting from either phosphorylation<sup className="footnote-ref">[<a href="#fn3" id="fnref3"></a>]</sup> or, for GPCRs, ligand binding<sup className="footnote-ref">[<a href="#fn4" id="fnref4"></a>]</sup>. So many of the previous generation's problems don't appear to have gone away, and success in more recently accessible use cases seems to be driven primarily by memorization.</p>

          <h4>NMR ensembles aren't thermodynamic ensembles</h4>
          <p><em>This was pointed out by Prof. Kresten Lindorff-Larsen on <a href="https://bsky.app/profile/lindorfflarsen.bsky.social/post/3loe4j5tbtc26"  target="_blank" rel="noopener noreferrer">Bluesky</a></em></p>
          <p>The vast majority of protein structures in the PDB are determined by X-ray crystallography or cryo-EM as independent models, and any ambiguities are captured by omitting certain atoms, such as terminal atoms of solvent-exposed arginines and lysines. NMR is another matter- instead of returning electron density in Cartesian space, the technique instead provides local restraints, such as short interatomic distances, from which full-atom models can be built. These data describe both some aspects of a protein's dynamics, but they're also incomplete, so ensembles of models are often made to capture both conformational heterogeneity implied by the restraints and the lack of global information. In the end, each model is an independent solution to the optimization problem that is structure prediction using NMR data.</p>

          <MolstarViewer
            customPdbUrl="https://files.rcsb.org/view/1X1M.pdb"
            options={nmrViewerOptions}
          />
          <p style={{ textAlign: 'center', fontStyle: 'italic', fontSize: '0.9em' }}>
            NMR structure of Ubiquitin (PDB: 1X1M)
          </p>

          <p>This came up because the team developing the protein folding neural network OpenComplex2<sup className="footnote-ref">[<a href="#fn5" id="fnref5"></a>]</sup> treat these ensembles of independently generated models as a thermodynamic ensemble reflecting the equilibrium dynamics of proteins in solution. They do not<sup className="footnote-ref">[<a href="#fn6" id="fnref6"></a>]</sup>, and there's probably little to no link between being able to reproduce these movements and learning the underlying forces driving protein dynamics. It seems that the most optimal test set is the nearly fifteen-year-old set of fast-folding proteins from D.E. Shaw<sup className="footnote-ref">[<a href="#fn7" id="fnref7"></a>]</sup>, which likely covers the full conformational landscape of these proteins, and has been used by the authors of BioEmu<sup className="footnote-ref">[<a href="#fn8" id="fnref8"></a>]</sup>, aSAMt<sup className="footnote-ref">[<a href="#fn9" id="fnref9"></a>]</sup>, and probably others.</p>

          <h4>A deep dive into protein language model scaling laws</h4>
          <p>Dr Pascal Notin wrote a <a href="https://substack.com/home/post/p-163085988" target="_blank" rel="noopener noreferrer">fantastic summary</a> of how, <a href="https://www.theverge.com/2024/12/12/24318650/chatgpt-openai-history-two-year-anniversary" target="_blank" rel="noopener noreferrer">as has been pointed out in tech media for natural language LLMs</a>, protein language models don't benefit from scaling beyond a certain point, approximately three to four billion parameters. Instead, performance improvements arise from including alternate modalities, such as multiple sequence alignments (which inform function) and structure (which informs binding and stability). The observation of plateauing performance in the single-digit billion parameter range corresponds with estimates made by Zhang et al last year<sup className="footnote-ref">[<a href="#fn11" id="fnref11"></a>]</sup> based on the number of protein domains and their size, despite improved pre-training performance<sup className="footnote-ref">[<a href="#fn12" id="fnref12"></a>]</sup>. This angle of memorization is probably the most likely (and seems to explain why low-perplexity predictions from large protein language models tend to be less useful for variant effect prediction<sup className="footnote-ref">[<a href="#fn13" id="fnref13"></a>]</sup>). </p>
          
          <div
            className="substack-post-embed"
            style={{
                margin: '2rem 0',
                width: '100%',
            }}
            ><p lang="en">Have We Hit the Scaling Wall for Protein Language Models? by Pascal Notin</p><p>Beyond Scaling: What Truly Works in Protein Fitness Prediction</p><a data-post-link href="https://pascalnotin.substack.com/p/have-we-hit-the-scaling-wall-for">Read on Substack</a></div><script async src="https://substack.com/embedjs/embed.js" charset="utf-8"></script>

          <p>Despite the consensus here, I still don't have a satisfactory mental model for why structure prediction neural networks that use these larger language models are able to correctly predict the structures of <em>de novo</em> designed proteins. It isn't clear to me how well these are predicted if smaller language models are used instead.</p>

          <FootnoteList
            citationsData={citationsData}
            idToNumberMap={idToNumberMap}
            isLoading={isLoading}
            error={error}
          />
        </div>
      );
    };
    return <ContentComponent />;
  })(),
};